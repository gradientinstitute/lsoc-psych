{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0802d8c-a677-42cb-8b09-f1024c082a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import fitting as fit\n",
    "import data_utils as dat\n",
    "import vis\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da3f65-8481-4573-a94b-c7cb44aec7a0",
   "metadata": {},
   "source": [
    "# Train/Test Splits\n",
    "\n",
    "Problem - how do we extract the \"interesting interval\" across different models and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40760652-c7c3-47be-a41e-857037b42209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available sizes: ['14m', '31m', '70m', '160m', '410m', '1b']\n",
    "msizes = ['160m', '410m']\n",
    "tasks = [\"arxiv\", \"wikipedia_en\"]\n",
    "functions = {\n",
    "    \"Power Law\": fit.ShiftedPowerLaw,\n",
    "    \"Exponential\": fit.ShiftedExponential,\n",
    "    # \"Power Law (4P)\": cf.DoubleShiftedPowerLaw,\n",
    "    # \"ExpExp\": cf.DoubleExponential,\n",
    "}\n",
    "\n",
    "# TODO: we need this for every model (and maybe for every model/task combination)\n",
    "step_start = 512  # are we cropping too early?\n",
    "step_end = 80000\n",
    "step_cutoff = 10000\n",
    "\n",
    "for msize in msizes:\n",
    "    df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "\n",
    "    for task in tasks:\n",
    "        trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "        \n",
    "        train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "        fig = go.Figure()\n",
    "        vis.plot_split(fig, train, test)\n",
    "\n",
    "        # Now plot all the models:\n",
    "        for model_name, model in functions.items():\n",
    "            \n",
    "            result = fit.min_fit(train.x, train.y, model)\n",
    "            vis.plot_result(fig, trace.x, result, name=model_name)\n",
    "\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "            xaxis_title='LLC / 100',\n",
    "            yaxis_title='Loss',\n",
    "            width=900, height=600,\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3e75b-616f-4c63-b880-47eeb17e1366",
   "metadata": {},
   "source": [
    "# The plots in shifted space don't look so good\n",
    "\n",
    "While the log(y-y*) makes the fit look amazing, its using the function's parameters to distort the space so of course it looks amazing.\n",
    "However, it can make the unseen data look worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f4f323-6a3e-4e33-9868-a89788424762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots in shifted space only make sense for the power law\n",
    "msizes = ['160m', '410m']\n",
    "tasks = [\"arxiv\", \"wikipedia_en\"]\n",
    "function = fit.ShiftedPowerLaw\n",
    "\n",
    "# TODO: we need this for every model (and maybe for every model/task combination)\n",
    "step_start = 512  # are we cropping too early?\n",
    "step_end = 80000\n",
    "step_cutoff = 10000\n",
    "\n",
    "for msize in msizes:\n",
    "    df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "\n",
    "    for task in tasks:\n",
    "        trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "        train, test = dat.split(trace, step_cutoff)\n",
    "        result = fit.min_fit(train.x, train.y, fit.ShiftedPowerLaw2)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        shift = result.params_dict  # contains y*\n",
    "        vis.plot_split(fig, train, test, shift=result.params_dict)\n",
    "        vis.plot_result(fig, trace.x, result, name=\"Power law fit\", shift=result.params_dict)\n",
    "\n",
    "        # As we're using shifted powerlaw, we should go back to logspace\n",
    "        fig.update_xaxes(title_text=\"L - L*\", type=\"log\")\n",
    "        fig.update_yaxes(title_text=\"LLC - LLC*\", type=\"log\")\n",
    "\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "            xaxis_title='LLC / 100',\n",
    "            yaxis_title='Loss',\n",
    "            width=900, height=600,\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a578d-4485-4b5e-bcb8-f948e190de1a",
   "metadata": {},
   "source": [
    "# Do fit methods make a difference?\n",
    "\n",
    "### ANSWER: a bit but not so much as to change which functional form is preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe324cf-00b8-48b3-be40-ca6fa67964b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msize=\"410m\"\n",
    "df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "tasks = [\"arxiv\", \"wikipedia_en\", \"full\"] \n",
    "functions = {\n",
    "    \"Power Law\": fit.ShiftedPowerLaw2,\n",
    "    \"Exponential\": fit.ShiftedExponential,\n",
    "}\n",
    "fit_methods = {\n",
    "    \"minimize\": fit.min_fit,\n",
    "    \"ODR\": fit.odr_fit,\n",
    "}\n",
    "hk_noise = {\n",
    "   \"rel_noise\": True,\n",
    "    \"\": False,\n",
    "}\n",
    "\n",
    "for task in tasks:\n",
    "    trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "    train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    vis.plot_split(fig, train, test)\n",
    "\n",
    "    for model_name, model in functions.items():\n",
    "        for fit_name, fit_fn in fit_methods.items():\n",
    "            for rel_name, rel_noise in hk_noise.items():\n",
    "                result = fit_fn(train.x, train.y, model, rel_noise=rel_noise)\n",
    "                vis.plot_result(\n",
    "                    fig, trace.x, result,\n",
    "                    name=f\"{model_name} {fit_name} {rel_name}\"\n",
    "                )\n",
    "\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "        xaxis_title='LLC / 100',\n",
    "        yaxis_title='Loss',\n",
    "        width=900, height=600,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e463ca-5c45-4039-8c69-d8c2faf0b3e9",
   "metadata": {},
   "source": [
    "# Can we use pcov to estimate uncertainty?\n",
    "Yes but its an approximation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33735c-1466-4456-831f-2787a347240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available sizes: ['14m', '31m', '70m', '160m', '410m', '1b']\n",
    "msizes = ['160m', '410m']\n",
    "tasks = [\"arxiv\", \"wikipedia_en\"]\n",
    "functions = {\n",
    "    \"Power Law\": fit.ShiftedPowerLaw,\n",
    "    \"Exponential\": fit.ShiftedExponential,\n",
    "    # \"Power Law (4P)\": cf.DoubleShiftedPowerLaw,\n",
    "    # \"ExpExp\": cf.DoubleExponential,\n",
    "}\n",
    "\n",
    "# TODO: we need this for every model (and maybe for every model/task combination)\n",
    "step_start = 512  # are we cropping too early?\n",
    "step_end = 80000\n",
    "step_cutoff = 10000\n",
    "\n",
    "colors = vis.assign_cols(functions)\n",
    "\n",
    "\n",
    "for msize in msizes:\n",
    "    df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "\n",
    "    for task in tasks:\n",
    "        trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "        \n",
    "        train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "        fig = go.Figure()\n",
    "        vis.plot_split(fig, train, test)\n",
    "\n",
    "        # Now plot all the models:\n",
    "        for model_name, model in functions.items():\n",
    "            \n",
    "            result = fit.min_fit(train.x, train.y, model)\n",
    "           \n",
    "            _, y_test_mu, y_test_std = result.sample(test.x, 30)\n",
    "            llh = fit.normal_log_likelihood(test.y, y_test_mu, y_test_std)\n",
    "\n",
    "            vis.sample_result(fig, trace.x, result, colors[model_name],\n",
    "                             name=f\"{model_name}: LLH~={llh:.1f}\")\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "            xaxis_title='LLC / 100',\n",
    "            yaxis_title='Loss',\n",
    "            width=900, height=600,\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0058aa7-fae2-4456-8fa8-bcc6927e20f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
