{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0802d8c-a677-42cb-8b09-f1024c082a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import fitting as fit\n",
    "import data_utils as dat\n",
    "import vis\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da3f65-8481-4573-a94b-c7cb44aec7a0",
   "metadata": {},
   "source": [
    "# Train/Test Splits\n",
    "\n",
    "Problem - how do we extract the \"interesting interval\" across different models and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf06b6-837d-42fb-a0e3-d32617256261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment\n",
    "fit_method = fit.min_fit  # odr_fit is also an option\n",
    "mcode = '410m-dense'\n",
    "step_start = 2000  # As long as it is consistent\n",
    "step_end = 80000\n",
    "step_cutoff = 20000  # TODO: investigate if this point is in the train or the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40760652-c7c3-47be-a41e-857037b42209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tasks = [\"arxiv\", \"pile-cc\"]\n",
    "functions = [\n",
    "    fit.DoubleOffsetPowerLaw,  # The first model sets the shift parameters for the other models\n",
    "    fit.OffsetPowerLaw,\n",
    "    fit.OffsetExponential,\n",
    "    fit.Cubic,\n",
    "]\n",
    "\n",
    "df_llc, df_loss = dat.load_dfs(mcode, data_path=\"data\")\n",
    "msize = mcode.split(\"-\")[0]\n",
    "tasks = df_llc.columns\n",
    "colors = vis.assign_cols(tasks)  # deal with wikipedia\n",
    "\n",
    "# Accumulate plots and reports ===================\n",
    "# Make a 2-column layout for each function\n",
    "titles = []\n",
    "for f in functions:\n",
    "    for s in [\"linear\", \"log\"]:\n",
    "        # titles.append(f\"Pythia-{msize} {f.name} ({s})\")\n",
    "        titles.append(f.name)  # f\"Pythia-{msize} {f.name} ({s})\")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=len(functions), cols=2,\n",
    "    subplot_titles=titles,\n",
    "    horizontal_spacing=0.1,\n",
    "    vertical_spacing=0.1,\n",
    ")\n",
    "\n",
    "report = []  # the set of reports\n",
    "\n",
    "for task in tasks:\n",
    "    color = colors[task]\n",
    "    trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "    train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "    # Cache fits so we can see the other methods and mark the \"winner\"\n",
    "    scores = []  # primary score\n",
    "    results = []  # fit result\n",
    "    \n",
    "    # Now plot all the models:\n",
    "    for f_ind, function in enumerate(functions):        \n",
    "\n",
    "        # See the future for plotting and init purposes only\n",
    "        oracle = fit_method(trace.x, trace.y, function)\n",
    "        if f_ind == 0:\n",
    "            # Use the same plotting projection as in the set of plots - the full fit on the double shifted\n",
    "            assert function is fit.DoubleOffsetPowerLaw, \"You've got the wrong axis labels for this projection\"\n",
    "            shift = oracle.params_dict\n",
    "            \n",
    "        # Fit the result\n",
    "        # not technically cheating but feels improper\n",
    "        # result = fit_method(train.x, train.y, function, par0=oracle.params)\n",
    "\n",
    "        # Clean fit (still depends on initial params a lot)\n",
    "        # maybe multi-start init would be an option\n",
    "        result = fit_method(train.x, train.y, function)\n",
    "\n",
    "        # Evaluate result\n",
    "        row = {\n",
    "            \"Dataset\": task,\n",
    "            \"Function\": function.name,\n",
    "        }\n",
    "        y_pred = result.f(test.x)\n",
    "        rmse = fit.rmse(test.y, y_pred)\n",
    "        scores.append(rmse)\n",
    "        results.append(result)\n",
    "\n",
    "    # Then plot in a second iteration\n",
    "    for f_ind, function in enumerate(functions):\n",
    "        result = results[f_ind]\n",
    "        score = scores[f_ind]\n",
    "\n",
    "        if score == min(scores):  # lower is better\n",
    "            score_repr = f\"RMSE=<b>{score:.4f}</b> {task}\"\n",
    "        else:\n",
    "            score_repr = f\"RMSE={score:.4f} {task}\"\n",
    "        \n",
    "        # colour unseen data lighter for some visual distinction\n",
    "        color2 = vis.add_color(color)\n",
    "\n",
    "        # vis.plot_data(fig, train.x, train.y, train.s, color=color,\n",
    "        #                 name=\"Observed\", showlegend=False, subplot=subplot, size=5)\n",
    "        # vis.plot_data(fig, test.x, test.y, test.s, color=color2,\n",
    "        #               name=\"Heldout\", showlegend=False, subplot=subplot, size=5)\n",
    "        \n",
    "        for col in range(1, 3):\n",
    "            subplot = dict(row=f_ind+1, col=col)\n",
    "            use_shift = shift if col==2 else None\n",
    "            vis.plot_data(fig, train.x, train.y, train.s, color=color,\n",
    "                          showlegend=False, subplot=subplot, size=5, shift=use_shift)\n",
    "            vis.plot_data(fig, test.x, test.y, test.s, color=color2,\n",
    "                          showlegend=False, subplot=subplot, size=5, shift=use_shift)\n",
    "            vis.plot_result(fig, trace.x, result, name=score_repr, color=color, subplot=subplot, shift=use_shift,\n",
    "                           showlegend=col==2, legendgroup=function.name) #legend=legend_id)\n",
    "            fig.update_xaxes(title_text=r\"$\\text{Estimated and transformed LLC }\\,\\frac{1}{100}\\hat{\\lambda} -\\lambda^*$\", **subplot)\n",
    "            if col == 1:\n",
    "                fig.update_yaxes(title_text=r\"$\\text{Loss }L$\", **subplot)\n",
    "            elif col == 2:\n",
    "                fig.update_yaxes(title_text=r\"$\\text{Loss }L - L^*$\", **subplot)     \n",
    "       \n",
    "\n",
    "\n",
    "# compile report etc\n",
    "report = pd.DataFrame(report, index=range(len(report)))\n",
    "report.index.name=\"Pythia\" + msize\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    width=1200,\n",
    "    height=410* len(functions),\n",
    "    showlegend=True,\n",
    "    legend_tracegroupgap=180,  # annoying - have to eyeball this\n",
    ")\n",
    "fname = f\"plots/holdout_{msize}.pdf\"\n",
    "fig.write_image(fname)\n",
    "#fig.show()\n",
    "print(f\"Done. See {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c38de4-4f8d-4727-8632-37b6d0fdb120",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.s, test.s, df_llc.max(), df_loss.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b439ab-2891-4875-ae10-dd6632e43b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop execution\n",
    "assert False, \"Don't run all past this point\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a578d-4485-4b5e-bcb8-f948e190de1a",
   "metadata": {},
   "source": [
    "# Do fit methods make a difference?\n",
    "\n",
    "### ANSWER: a bit but not so much as to change which functional form is preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe324cf-00b8-48b3-be40-ca6fa67964b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msize=\"410m\"\n",
    "df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "tasks = [\"arxiv\", \"wikipedia_en\", \"full\"] \n",
    "functions = {\n",
    "    \"Power Law\": fit.ShiftedPowerLaw2,\n",
    "    \"Exponential\": fit.ShiftedExponential,\n",
    "}\n",
    "fit_methods = {\n",
    "    \"minimize\": fit.min_fit,\n",
    "    \"ODR\": fit.odr_fit,\n",
    "}\n",
    "hk_noise = {\n",
    "   \"rel_noise\": True,\n",
    "    \"\": False,\n",
    "}\n",
    "\n",
    "for task in tasks:\n",
    "    trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "    train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    vis.plot_split(fig, train, test)\n",
    "\n",
    "    for model_name, model in functions.items():\n",
    "        for fit_name, fit_fn in fit_methods.items():\n",
    "            for rel_name, rel_noise in hk_noise.items():\n",
    "                result = fit_fn(train.x, train.y, model, rel_noise=rel_noise)\n",
    "                vis.plot_result(\n",
    "                    fig, trace.x, result,\n",
    "                    name=f\"{model_name} {fit_name} {rel_name}\"\n",
    "                )\n",
    "\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "        xaxis_title='LLC / 100',\n",
    "        yaxis_title='Loss',\n",
    "        width=900, height=600,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e463ca-5c45-4039-8c69-d8c2faf0b3e9",
   "metadata": {},
   "source": [
    "# Can we use pcov to estimate uncertainty?\n",
    "Yes but its an approximation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33735c-1466-4456-831f-2787a347240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available sizes: ['14m', '31m', '70m', '160m', '410m', '1b']\n",
    "msizes = ['160m', '410m']\n",
    "tasks = [\"arxiv\", \"wikipedia_en\"]\n",
    "functions = {\n",
    "    \"Power Law\": fit.ShiftedPowerLaw,\n",
    "    \"Exponential\": fit.ShiftedExponential,\n",
    "\n",
    "}\n",
    "\n",
    "colors = vis.assign_cols(functions)\n",
    "\n",
    "\n",
    "for msize in msizes:\n",
    "    df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "\n",
    "    for task in tasks:\n",
    "        trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "        \n",
    "        train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "        fig = go.Figure()\n",
    "        vis.plot_split(fig, train, test)\n",
    "\n",
    "        # Now plot all the models:\n",
    "        for model_name, model in functions.items():\n",
    "            \n",
    "            result = fit.min_fit(train.x, train.y, model)\n",
    "           \n",
    "            _, y_test_mu, y_test_std = result.sample(test.x, 30)\n",
    "            llh = fit.normal_log_likelihood(test.y, y_test_mu, y_test_std)\n",
    "\n",
    "            vis.sample_result(fig, trace.x, result, colors[model_name],\n",
    "                             name=f\"{model_name}: LLH~={llh:.1f}\")\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "            xaxis_title='LLC / 100',\n",
    "            yaxis_title='Loss',\n",
    "            width=900, height=600,\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0058aa7-fae2-4456-8fa8-bcc6927e20f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
