{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0802d8c-a677-42cb-8b09-f1024c082a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import fitting as fit\n",
    "import data_utils as dat\n",
    "import vis\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da3f65-8481-4573-a94b-c7cb44aec7a0",
   "metadata": {},
   "source": [
    "# Train/Test Splits\n",
    "\n",
    "Problem - how do we extract the \"interesting interval\" across different models and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40760652-c7c3-47be-a41e-857037b42209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "fit_method = fit.min_fit  # fit.min_fit  # super robust, odr_fit also an option\n",
    "mcode = '410m-dense'\n",
    "step_start = 2000  # are we cropping too early?\n",
    "step_end = 80000\n",
    "step_cutoff = 20000  # TODO: investigate if this point is in the train or the test set\n",
    "\n",
    "#tasks = [\"arxiv\", \"pile-cc\"]\n",
    "functions = [\n",
    "    fit.DoubleOffsetPowerLaw,  # The first model sets the shift parameters for the other models\n",
    "    fit.OffsetPowerLaw,\n",
    "    fit.OffsetExponential,\n",
    "    fit.Cubic,\n",
    "]\n",
    "metrics = {\n",
    "    # \"R2_log\": fit.logspace_r2,\n",
    "    # \"R2_lin\": fit.r2_score,\n",
    "    \"RMSE\": fit.rmse,\n",
    "}\n",
    "\n",
    "df_llc, df_loss = dat.load_dfs(mcode, data_path=\"data\")\n",
    "msize = mcode.split(\"-\")[0]\n",
    "tasks = df_llc.columns\n",
    "colors = vis.assign_cols(tasks)  # deal with wikipedia\n",
    "\n",
    "# Accumulate plots and reports ===================\n",
    "# Make a 2-column layout for each function\n",
    "titles = []\n",
    "for f in functions:\n",
    "    for s in [\"linear\", \"log\"]:\n",
    "        # titles.append(f\"Pythia-{msize} {f.name} ({s})\")\n",
    "        titles.append(f.name)  # f\"Pythia-{msize} {f.name} ({s})\")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=len(functions), cols=2,\n",
    "    subplot_titles=titles,\n",
    "    horizontal_spacing=0.1,\n",
    "    vertical_spacing=0.1,\n",
    ")\n",
    "\n",
    "report = []  # the set of reports\n",
    "\n",
    "for task in tasks:\n",
    "    color = colors[task]\n",
    "    trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "    train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "    # Cache fits so we can see the other methods and mark the \"winner\"\n",
    "    scores = []  # primary score\n",
    "    results = []  # fit result\n",
    "    \n",
    "    # Now plot all the models:\n",
    "    for f_ind, function in enumerate(functions):        \n",
    "\n",
    "        # See the future for plotting and init purposes only\n",
    "        oracle = fit_method(trace.x, trace.y, function)\n",
    "        if f_ind == 0:\n",
    "            # Use the same plotting projection as in the set of plots - the full fit on the double shifted\n",
    "            shift = oracle.params_dict\n",
    "            \n",
    "        # Fit the result\n",
    "        result = fit_method(train.x, train.y, function, par0=oracle.params)\n",
    "\n",
    "        # Evaluate result\n",
    "        row = {\n",
    "            \"Dataset\": task,\n",
    "            \"Function\": function.name,\n",
    "        }\n",
    "        y_pred = result.f(test.x)\n",
    "        measures = {k: v(test.y, y_pred) for k, v in metrics.items()}\n",
    "        row.update(measures)\n",
    "        report.append(row) \n",
    "        scores.append(measures[\"RMSE\"])\n",
    "        results.append(result)\n",
    "\n",
    "    # Then plot in a second iteration\n",
    "    for f_ind, function in enumerate(functions):\n",
    "        result = results[f_ind]\n",
    "        score = scores[f_ind]\n",
    "\n",
    "        if score == min(scores):  # lower is better\n",
    "            score_repr = f\"RMSE=<b>{score:.4f}</b> {task}\"\n",
    "        else:\n",
    "            score_repr = f\"RMSE={score:.4f} {task}\"\n",
    "        \n",
    "        # colour unseen data lighter for some visual distinction\n",
    "        color2 = vis.add_color(color)\n",
    "\n",
    "        # vis.plot_data(fig, train.x, train.y, train.s, color=color,\n",
    "        #                 name=\"Observed\", showlegend=False, subplot=subplot, size=5)\n",
    "        # vis.plot_data(fig, test.x, test.y, test.s, color=color2,\n",
    "        #               name=\"Heldout\", showlegend=False, subplot=subplot, size=5)\n",
    "        \n",
    "        for col in range(1, 3):\n",
    "            subplot = dict(row=f_ind+1, col=col)\n",
    "            use_shift = shift if col==2 else None\n",
    "            vis.plot_data(fig, train.x, train.y, train.s, color=color,\n",
    "                          showlegend=False, subplot=subplot, size=5, shift=use_shift)\n",
    "            vis.plot_data(fig, test.x, test.y, test.s, color=color2,\n",
    "                          showlegend=False, subplot=subplot, size=5, shift=use_shift)\n",
    "            vis.plot_result(fig, trace.x, result, name=score_repr, color=color, subplot=subplot, shift=use_shift,\n",
    "                           showlegend=col==2, legendgroup=function.name) #legend=legend_id)\n",
    "            fig.update_xaxes(title_text=r\"$\\text{Estimated and transformed LLC }\\,\\frac{1}{100}\\hat{\\lambda}$\", **subplot)\n",
    "            if col == 1:\n",
    "                fig.update_yaxes(title_text=r\"$\\text{Loss }L$\", **subplot)\n",
    "            elif col == 2:\n",
    "                fig.update_yaxes(title_text=r\"$\\text{Loss }L - L^*$\", **subplot)     \n",
    "       \n",
    "\n",
    "\n",
    "# compile report etc\n",
    "report = pd.DataFrame(report, index=range(len(report)))\n",
    "report.index.name=\"Pythia\" + msize\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    width=1200,\n",
    "    height=410* len(functions),\n",
    "    showlegend=True,\n",
    "    legend_tracegroupgap=180,  # annoying - have to eyeball this\n",
    ")\n",
    "fname = f\"plots/holdout_{msize}.pdf\"\n",
    "fig.write_image(fname)\n",
    "#fig.show()\n",
    "print(f\"Done. See {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a69261-2c6c-4e4b-b524-4843d999b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_value(val, is_winner, is_rmse=False):\n",
    "    # Format to 3 decimal places\n",
    "    formatted = f\"{val:.3f}\"\n",
    "    if is_winner:\n",
    "        return f\"\\\\textbf{{{formatted}}}\"\n",
    "    return formatted\n",
    "\n",
    "def highlight_winners(df):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Process each unique dataset\n",
    "    for dataset in df['Dataset'].unique():\n",
    "        mask = df['Dataset'] == dataset\n",
    "        group = df[mask]\n",
    "        \n",
    "        # Find winners (max for R2, min for RMSE)\n",
    "        # winners_r2_log = group['R2_log'] == group['R2_log'].max()\n",
    "        # winners_r2_lin = group['R2_lin'] == group['R2_lin'].max()\n",
    "        winners_rmse = group['RMSE'] == group['RMSE'].min()\n",
    "        \n",
    "        # Format all values in the group\n",
    "        # result.loc[mask, 'R2_log'] = [format_value(val, win) \n",
    "        #                              for val, win in zip(group['R2_log'], winners_r2_log)]\n",
    "        # result.loc[mask, 'R2_lin'] = [format_value(val, win) \n",
    "        #                              for val, win in zip(group['R2_lin'], winners_r2_lin)]\n",
    "        result.loc[mask, 'RMSE'] = [format_value(val, win, is_rmse=True) \n",
    "                                   for val, win in zip(group['RMSE'], winners_rmse)]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735d734-db45-41a1-a12c-5791c20060f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bold the best fit!\n",
    "report2 = highlight_winners(report)\n",
    "display(report2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da6cce-30be-4832-81ed-d50fbfe6888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report2.to_latex(index=False).replace(\"R2_log\", \"R^2 (logspace)\").replace(\"R2_lin\", \"R^2\").replace(\"_\", \"\\\\_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb11dbe-e935-4227-909f-d28621f70aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8267cd9-8ae6-45d6-a171-a6f20d0a5d03",
   "metadata": {},
   "source": [
    "# Plot each function on the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77403b-ae27-490c-b8a9-d764cbdd5611",
   "metadata": {},
   "outputs": [],
   "source": [
    "msize = '410m-dense'\n",
    "step_start = 2000  # are we cropping too early?\n",
    "step_end = 80000\n",
    "step_cutoff = 20000\n",
    "fit_method = fit.odr_fit  # much better for \n",
    "#tasks = [\"arxiv\", \"pile-cc\"]\n",
    "functions = [\n",
    "    fit.ShiftedPowerLaw,\n",
    "    # fit.DoubleShiftedPowerLaw,\n",
    "    fit.ShiftedExponential,\n",
    "    fit.Cubic,\n",
    "]\n",
    "metrics = {\n",
    "    \"R2_log\": fit.logspace_r2,\n",
    "    \"R2_lin\": fit.r2_score,\n",
    "    \"RMSE\": fit.rmse,\n",
    "}\n",
    "\n",
    "df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "tasks = df_llc.columns\n",
    "colors = vis.assign_cols(tasks)  # deal with wikipedia\n",
    "\n",
    "# Accumulate plots and reports\n",
    "fig = go.Figure()\n",
    "reports = []  # the set of reports\n",
    "\n",
    "for task in tasks:\n",
    "    trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "    train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "    \n",
    "    vis.plot_split(fig, train, test)\n",
    "\n",
    "    # Now plot all the models:\n",
    "    for model_name, model in functions.items():\n",
    "        \n",
    "        result = fit.min_fit(train.x, train.y, model)\n",
    "        vis.plot_result(fig, trace.x, result, name=model_name)\n",
    "\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "        xaxis_title='LLC / 100',\n",
    "        yaxis_title='Loss',\n",
    "        width=900, height=600,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3e75b-616f-4c63-b880-47eeb17e1366",
   "metadata": {},
   "source": [
    "# The plots in shifted space don't look so good\n",
    "\n",
    "While the log(y-y*) makes the fit look amazing, its using the function's parameters to distort the space so of course it looks amazing.\n",
    "However, it can make the unseen data look worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f4f323-6a3e-4e33-9868-a89788424762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots in shifted space only make sense for the power law\n",
    "msizes = ['160m', '410m']\n",
    "tasks = [\"arxiv\", \"wikipedia_en\"]\n",
    "function = fit.ShiftedPowerLaw\n",
    "\n",
    "# TODO: we need this for every model (and maybe for every model/task combination)\n",
    "step_start = 512  # are we cropping too early?\n",
    "step_end = 80000\n",
    "step_cutoff = 10000\n",
    "\n",
    "for msize in msizes:\n",
    "    df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "\n",
    "    for task in tasks:\n",
    "        trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "        train, test = dat.split(trace, step_cutoff)\n",
    "        result = fit.min_fit(train.x, train.y, fit.ShiftedPowerLaw2)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        shift = result.params_dict  # contains y*\n",
    "        vis.plot_split(fig, train, test, shift=result.params_dict)\n",
    "        vis.plot_result(fig, trace.x, result, name=\"Power law fit\", shift=result.params_dict)\n",
    "\n",
    "        # As we're using shifted powerlaw, we should go back to logspace\n",
    "        fig.update_xaxes(title_text=\"L - L*\", type=\"log\")\n",
    "        fig.update_yaxes(title_text=\"LLC - LLC*\", type=\"log\")\n",
    "\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "            xaxis_title='LLC / 100',\n",
    "            yaxis_title='Loss',\n",
    "            width=900, height=600,\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a578d-4485-4b5e-bcb8-f948e190de1a",
   "metadata": {},
   "source": [
    "# Do fit methods make a difference?\n",
    "\n",
    "### ANSWER: a bit but not so much as to change which functional form is preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe324cf-00b8-48b3-be40-ca6fa67964b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msize=\"410m\"\n",
    "df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "tasks = [\"arxiv\", \"wikipedia_en\", \"full\"] \n",
    "functions = {\n",
    "    \"Power Law\": fit.ShiftedPowerLaw2,\n",
    "    \"Exponential\": fit.ShiftedExponential,\n",
    "}\n",
    "fit_methods = {\n",
    "    \"minimize\": fit.min_fit,\n",
    "    \"ODR\": fit.odr_fit,\n",
    "}\n",
    "hk_noise = {\n",
    "   \"rel_noise\": True,\n",
    "    \"\": False,\n",
    "}\n",
    "\n",
    "for task in tasks:\n",
    "    trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "    train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    vis.plot_split(fig, train, test)\n",
    "\n",
    "    for model_name, model in functions.items():\n",
    "        for fit_name, fit_fn in fit_methods.items():\n",
    "            for rel_name, rel_noise in hk_noise.items():\n",
    "                result = fit_fn(train.x, train.y, model, rel_noise=rel_noise)\n",
    "                vis.plot_result(\n",
    "                    fig, trace.x, result,\n",
    "                    name=f\"{model_name} {fit_name} {rel_name}\"\n",
    "                )\n",
    "\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "        xaxis_title='LLC / 100',\n",
    "        yaxis_title='Loss',\n",
    "        width=900, height=600,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e463ca-5c45-4039-8c69-d8c2faf0b3e9",
   "metadata": {},
   "source": [
    "# Can we use pcov to estimate uncertainty?\n",
    "Yes but its an approximation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33735c-1466-4456-831f-2787a347240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available sizes: ['14m', '31m', '70m', '160m', '410m', '1b']\n",
    "msizes = ['160m', '410m']\n",
    "tasks = [\"arxiv\", \"wikipedia_en\"]\n",
    "functions = {\n",
    "    \"Power Law\": fit.ShiftedPowerLaw,\n",
    "    \"Exponential\": fit.ShiftedExponential,\n",
    "    # \"Power Law (4P)\": cf.DoubleShiftedPowerLaw,\n",
    "    # \"ExpExp\": cf.DoubleExponential,\n",
    "}\n",
    "\n",
    "# TODO: we need this for every model (and maybe for every model/task combination)\n",
    "step_start = 512  # are we cropping too early?\n",
    "step_end = 80000\n",
    "step_cutoff = 10000\n",
    "\n",
    "colors = vis.assign_cols(functions)\n",
    "\n",
    "\n",
    "for msize in msizes:\n",
    "    df_llc, df_loss = dat.load_dfs(msize, data_path=\"data\")\n",
    "\n",
    "    for task in tasks:\n",
    "        trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "        \n",
    "        train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "        fig = go.Figure()\n",
    "        vis.plot_split(fig, train, test)\n",
    "\n",
    "        # Now plot all the models:\n",
    "        for model_name, model in functions.items():\n",
    "            \n",
    "            result = fit.min_fit(train.x, train.y, model)\n",
    "           \n",
    "            _, y_test_mu, y_test_std = result.sample(test.x, 30)\n",
    "            llh = fit.normal_log_likelihood(test.y, y_test_mu, y_test_std)\n",
    "\n",
    "            vis.sample_result(fig, trace.x, result, colors[model_name],\n",
    "                             name=f\"{model_name}: LLH~={llh:.1f}\")\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f'Predictions after step {step_cutoff} - {task} @ {msize}',\n",
    "            xaxis_title='LLC / 100',\n",
    "            yaxis_title='Loss',\n",
    "            width=900, height=600,\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0058aa7-fae2-4456-8fa8-bcc6927e20f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
