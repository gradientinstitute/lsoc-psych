{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c79abb-cc8f-4a63-a262-15541aec7615",
   "metadata": {},
   "source": [
    "# Power law coefficients\n",
    "## Searching for an interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314f016-8a9b-45f8-b472-3ecfa9984589",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import fitting as fit\n",
    "import data_utils as dat\n",
    "import pandas as pd\n",
    "import vis\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4545525-c6e1-44a3-9771-a080a0a1af5f",
   "metadata": {},
   "source": [
    "## Coefficient Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c72c3d-4f99-4736-a393-a6e89cde277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# be conservative initially\n",
    "_start = 2000  # we believe this should be in-common\n",
    "\n",
    "\n",
    "# Is there any pattern over model size?\n",
    "\n",
    "df_llc, df_loss = dat.load_dfs(\"160m\", data_path=\"data\")  # I believe 160m has a full task list\n",
    "\n",
    "\n",
    "\n",
    "# m_codes = [\n",
    "    # ('14m', _start, 20000),\n",
    "    # ('31m',_start, 20000),\n",
    "    # ('70m', _start, 20000),\n",
    "    # ('160m', _start, 30000),\n",
    "    # ('410m-dense', _start, 80000),\n",
    "#     ('1b', _start, 30000),\n",
    "# ]\n",
    "\n",
    "# m_code, start_step, end_step = ('1b', _start, 30000)\n",
    "m_code, start_step, end_step = ('160m', _start, 30000)\n",
    "\n",
    "\n",
    "rs = {}\n",
    "Ls = {}\n",
    "msize = m_code.split(\"-\")[0]\n",
    "df_llc, df_loss = dat.load_dfs(m_code, data_path=\"data\")\n",
    "\n",
    "tasks = df_llc.columns\n",
    "colors = vis.assign_cols(tasks)\n",
    "task_xs = {t:[] for t in tasks}\n",
    "task_ys = {t:[] for t in tasks}\n",
    "task_siz = {t:[] for t in tasks}\n",
    "\n",
    "# Not every model has a full task list\n",
    "for task in df_llc.columns:\n",
    "    trace = dat.trim_trace(df_llc, df_loss, task, start_step, end_step)\n",
    "    result = fit.min_fit(trace.x, trace.y, fit.OffsetPowerLaw)\n",
    "    pars = result.params_dict\n",
    "    desc = f\"{msize}-{task}\"\n",
    "    rs[task] = x = float(pars[\"r\"])\n",
    "    Ls[task] = y = float(pars[\"y*\"])\n",
    "    \n",
    "    task_xs[task].append(x)\n",
    "    task_ys[task].append(y)\n",
    "    task_siz[task].append(msize.upper())\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for task in tasks:\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=task_xs[task],\n",
    "        y=task_ys[task],\n",
    "        customdata=task_siz[task],\n",
    "        marker=dict(\n",
    "            color=colors[task],\n",
    "            size=6,\n",
    "        ),\n",
    "        mode='markers+lines+text',\n",
    "        name=task,\n",
    "        text=task, #task_siz[task],\n",
    "        textfont=dict(size=8),\n",
    "        textposition='top right',\n",
    "        hovertemplate=\"Model: %{customdata}<br><extra></extra>\",\n",
    "    ))\n",
    "fig.update_xaxes(title_text=\"r\")\n",
    "fig.update_yaxes(title_text=\"L*\")\n",
    "fig.update_layout(title=\"\", width=800, height=600)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71668c9-a278-4850-82f5-688eb936fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siphon off the graph results\n",
    "\n",
    "# Create DataFrame from the dictionaries\n",
    "coeffs = pd.DataFrame({\n",
    "    'R': rs,\n",
    "    'L': Ls\n",
    "}).reset_index().rename(columns={'index': 'dataset'})\n",
    "coeffs.set_index(\"dataset\", inplace=True)\n",
    "coeffs.drop(index=[\"full\"], inplace=True)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d715fb0-2289-4f7e-b4ec-15174a22da8a",
   "metadata": {},
   "source": [
    "## Try to link coefficients to the dataset\n",
    "lossless compression is a good proxy for \"memorisation\", yes? Look to zip, or facebook's dictionary training zstd.\n",
    "But would it make more sense to look at the single file compression ratio, or the mutual information with the pile?\n",
    "\n",
    "Usage:\n",
    "\n",
    "### Solid compression\n",
    "`cat arxiv/* | zstd -13 -c > archive.solid.zst`\n",
    "\n",
    "### Separate file compression\n",
    "`zstd -13 -c arxiv/* > archive-pieces.zst`\n",
    "\n",
    "### Extract a (text) dictionary\n",
    "`zstd --train -o arxiv.dict arxiv/`\n",
    "\n",
    "### Comptess with the dictionary\n",
    "`zstd -13 -d arxiv.dict -c arxiv/* > archive-dicted.zst`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71d5dc-fd0c-4907-afe2-1f5e72954870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('samples') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8ec7b-e5a7-4d1d-bf00-d291991360f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Train mushes a bunch of samples into a single file that should fit in context\n",
    "ref = \"full\"  # \"train\"  # Which dataset to refer to\n",
    "#cmd = f\"cat {ref}/* > {ref}.dict\"  # for full memory!\n",
    "max_dict=\"1M\"  # Will comfortably fit inside the context window\n",
    "split = 5000  # it truncates large individual samples with a warning - but we can split them up\n",
    "cmd = f\"zstd --train --maxdict={max_dict} -B{split} -o {ref}.dict {ref}/*\"\n",
    "print(cmd)\n",
    "subprocess.run(cmd, shell=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d5f57-2eb7-485f-ae2c-5f53f526d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress each task against the training dictionary\n",
    "# - one sample at a time so it doesn't peek at context from other samples\n",
    "pile_composition = {\n",
    "    \"arxiv\": 120.71,\n",
    "    \"pile-cc\": 243.87,\n",
    "    \"dm_mathematics\": 16.63,\n",
    "    \"enron_emails\": 1.89,\n",
    "    \"freelaw\": 82.39,\n",
    "    \"github\": 102.18,\n",
    "    \"hackernews\": 8.38,\n",
    "    \"nih_exporter\": 4.07,\n",
    "    \"pubmed_abstracts\": 41.37,\n",
    "    \"pubmed_central\": 193.86,\n",
    "    \"stackexchange\": 69.14,\n",
    "    \"uspto_backgrounds\": 49.19,\n",
    "    \"wikipedia_en\": 20.54,\n",
    "    # \"(missing)books3\": 162.61,\n",
    "    # \"(missing)openwebtext2\": 134.80,\n",
    "    # \"(missing)gutenberg\": 29.20,\n",
    "    # \"(missing)opensubtitles\": 20.91,\n",
    "    # \"(missing)ubuntu_irc\": 11.84,\n",
    "    # \"(missing)bookcorpus2\": 10.15,\n",
    "    # \"(missing)europarl\": 9.85,\n",
    "    # \"(missing)youtube_subtitles\": 8.02,\n",
    "    # \"(missing)philpapers\": 5.11\n",
    "}\n",
    "\n",
    "\n",
    "for task in pile_composition:  # don't do full\n",
    "    print(f\"Compressing task: {task}\")\n",
    "    # Do it with and without a dictionary....\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35898982-68e1-495f-9b19-5d437b0d803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "level = \"13\"  # zstd compression level\n",
    "\n",
    "for task in pile_composition:\n",
    "    print(f\"Compress {task} with dictionary:\")\n",
    "    cmd = f\"zstd -{level} -D {ref}.dict -c {task}/* > {task}-{ref}.zst\"\n",
    "    print(cmd)\n",
    "    assert not subprocess.run(cmd, shell=True).returncode\n",
    "\n",
    "    print(f\"Compress {task} without dictionary:\")\n",
    "    cmd = f\"zstd -{level} -c {task}/* > {task}-indiv.zst\"\n",
    "    print(cmd)\n",
    "    assert not subprocess.run(cmd, shell=True).returncode\n",
    "\n",
    "    print(f\"Compress {task} with solid (inter-sample) patterns:\")\n",
    "    cmd = f\"cat {task}/* | zstd -{level} -c > {task}-solid.zst\"\n",
    "    assert not subprocess.run(cmd, shell=True).returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f91535-daec-4c40-814b-e9ae55a19efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the file sizes - we're looking for something (anything!!!)\n",
    "# that correlates with the power law coefficients\n",
    "MB = 1000000\n",
    "# Train mushes a bunch of samples into a single file that should fit in context\n",
    "ref = \"full\"  # Which dataset to refer to\n",
    "results = []\n",
    "\n",
    "for task in pile_composition:\n",
    "    \n",
    "    # Get original size of all files in task folder\n",
    "    cmd = f\"du -sb {task}\"\n",
    "    console = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    raw = int(console.stdout.split(\"\\t\")[0])  # uncompressed size (~25mb)\n",
    "    \n",
    "    # Get filesize stats\n",
    "    results.append({\n",
    "        'task': task,\n",
    "        'baseline': os.path.getsize(f\"{task}-indiv.zst\") / raw,\n",
    "        'solid': os.path.getsize(f\"{task}-solid.zst\") / raw,\n",
    "        'dictionary': os.path.getsize(f\"{task}-{ref}.zst\") / raw,\n",
    "    })\n",
    "    \n",
    "# Create dataframe and sort by compression ratio\n",
    "df = pd.DataFrame(results)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "df.set_index(\"task\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0073abcc-a921-43f3-95b7-a26e110e105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [t for t in pile_composition]\n",
    "reps = np.array([pile_composition[t] for t in tasks])\n",
    "reps_s = pd.Series(reps / np.max(reps), index=tasks)\n",
    "reps_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337319b3-a4eb-405f-abd0-f368e44c7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Components\n",
    "#df[\"mutual\"] = df[\"indiv\"] - df[\"dict\"]  # How much informaion you gain by compressing with the \"training\" dictionary\n",
    "df[\"ineffectiveness\"] = df[\"dictionary\"] / df[\"baseline\"]  # How much informaion you gain by compressing with the \"training\" dictionary\n",
    "df[\"effectiveness\"] = df[\"baseline\"] / df[\"dictionary\"]\n",
    "df[\"irredundancy\"] = df[\"solid\"] / df[\"baseline\"]  # How much information you gain by compressing with an \"ideal\" dictionary \n",
    "df[\"redundancy\"] = df[\"baseline\"] / df[\"solid\"]\n",
    "df[\"dict advantage\"] = df[\"baseline\"] - df[\"dictionary\"]\n",
    "df[\"solid advantage\"] = df[\"baseline\"] - df[\"solid\"]\n",
    "# # And consider ones normalised by size in the dataset?\n",
    "# df[\"unique_scaled\"] = df[\"mutual\"] * reps_s\n",
    "# df[\"indiv_scaled\"] = df[\"indiv\"] * reps_s\n",
    "# df[\"ideal_scaled\"] = df[\"ideal\"] * reps_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040dc09-8a31-411c-8ec7-ded0043a19f4",
   "metadata": {},
   "source": [
    "# Any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a4a48-7714-4fa0-8abf-db7e84438080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate(df_a, df_b, method='pearson'):\n",
    "    \n",
    "    # Initialize correlation matrix\n",
    "    correlations = pd.DataFrame(\n",
    "        index=df_a.columns,\n",
    "        columns=df_b.columns,\n",
    "        dtype=float\n",
    "    )\n",
    "    \n",
    "    # Calculate correlations\n",
    "    for col_a in df_a.columns:\n",
    "        for col_b in df_b.columns:\n",
    "            # Handle non-numeric columns\n",
    "            if not pd.api.types.is_numeric_dtype(df_a[col_a]) or \\\n",
    "               not pd.api.types.is_numeric_dtype(df_b[col_b]):\n",
    "                correlations.loc[col_a, col_b] = np.nan\n",
    "                continue\n",
    "                \n",
    "            # Calculate correlation\n",
    "            correlation = df_a[col_a].corr(df_b[col_b], method=method)\n",
    "            correlations.loc[col_a, col_b] = correlation\n",
    "            \n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ebbb81-c4e3-4851-afbc-c92b6683050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = correlate(coeffs, df)\n",
    "print(corrs.to_markdown(\n",
    "    floatfmt=\".2f\",  # Format floats to 2 decimal places\n",
    "))\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de0e94-af34-46dc-ab83-590b9a4220d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import vis\n",
    "# Create the scatter plot\n",
    "colors = vis.assign_cols(tasks)\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=df[\"ineffectiveness\"],\n",
    "    y=coeffs[\"R\"],\n",
    "    #text=tasks,  # Using the common index for annotations\n",
    "    color=tasks,\n",
    "    color_discrete_map=colors,\n",
    "    labels={\n",
    "        'y': 'R Coefficient',\n",
    "        'x': 'Dictionary (in)effectiveness',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Adjust text position and layout\n",
    "fig.update_traces(\n",
    "    textposition='top center',  # Position text above points\n",
    "    mode='markers+text'  # Show both markers and text\n",
    ")\n",
    "fig.update_layout(width=800, height=600, title=\"Exponent vs dictionary effectiveness\")\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca5e105-1edb-45a7-b779-0752f3d8b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = vis.assign_cols(tasks)\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=df[\"ineffectiveness\"],\n",
    "    y=coeffs[\"L\"],\n",
    "    #text=tasks,  # Using the common index for annotations\n",
    "    color=tasks,\n",
    "    color_discrete_map=colors,\n",
    "    labels={\n",
    "        'y': 'L Coefficient',\n",
    "        'x': 'Dictionary (In)effectiveness',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Adjust text position and layout\n",
    "fig.update_traces(\n",
    "    textposition='top center',  # Position text above points\n",
    "    mode='markers+text'  # Show both markers and text\n",
    ")\n",
    "fig.update_layout(width=800, height=600, title=\"L* vs dictionary effectiveness\")\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd2dd2-742f-4caa-848e-188c9b8ea37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3eca60-20cf-40e4-8d33-20a6b169d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(compression.index) - set(coeffs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b8cf2-a926-4516-9f10-fa8cac184c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Dictionary sizes to test (in KB)\n",
    "sizes = np.exp(np.linspace(np.log(64), np.log(8192), 10)).round().astype(int)\n",
    "results = []\n",
    "train_name = \"train\"\n",
    "\n",
    "for size in sizes:\n",
    "    print(f\"Processing {size}KB dictionary...\")\n",
    "    size_bytes = size * 1024\n",
    "    dict_name = f\"train_{size}k.zdict\"\n",
    "    archive_name = f\"train_{size}k.zst\"\n",
    "\n",
    "    # Train dictionary\n",
    "    subprocess.run(f\"zstd --train --maxdict={size_bytes} -o {dict_name} {train_name}/*\", \n",
    "                  shell=True, \n",
    "                  stderr=subprocess.DEVNULL)\n",
    "\n",
    "    # Compress files\n",
    "    subprocess.run(f\"zstd -D {dict_name} -c {train_name}/* > {archive_name}\", \n",
    "                  shell=True, \n",
    "                  stderr=subprocess.DEVNULL)\n",
    "\n",
    "    # Get sizes\n",
    "    dict_size = os.path.getsize(dict_name)\n",
    "    compressed_size = os.path.getsize(archive_name)\n",
    "    total_size = dict_size + compressed_size\n",
    "\n",
    "    results.append({\n",
    "        'dict_size_kb': size,\n",
    "        'dict_size_bytes': dict_size,\n",
    "        'compressed_size_bytes': compressed_size,\n",
    "        'total_size_bytes': total_size\n",
    "    })\n",
    "\n",
    "    # Cleanup\n",
    "    os.remove(dict_name)\n",
    "    os.remove(archive_name)\n",
    "\n",
    "# Create dataframe and display results\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7baf0-e9ed-4715-b1ca-34e019762254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "MB = 1e6\n",
    "\n",
    "\n",
    "def get_zip_size(name, base=None, compression_level=9):\n",
    "    \"\"\"Get size of zip file containing given files\"\"\"\n",
    "    in_file = os.path.join(data_dir, name + \".dat\")\n",
    "    \n",
    "    if base:\n",
    "        # copy over the base first.... (easier than recompressing...)\n",
    "        base_file = os.path.join(data_dir, base + \".xz\")\n",
    "        assert os.path.exists(base_file)\n",
    "        # shutil.copy(base_file, zip_file)  # compress it once, and reuse later!\n",
    "        name = f\"{name}+{base}\"\n",
    "    else:\n",
    "        base_file = \"\"\n",
    "        \n",
    "    zip_file = os.path.join(data_dir, name + \".xz\")\n",
    "    \n",
    "    if not os.path.exists(zip_file): \n",
    "        print(\"Compressing \" + name)\n",
    "        # The \"ms\" is crucial - look up \"solid\" compression, one is an archive of zips, the other is a zip of archives\n",
    "        subprocess.run(f\"cat {base_file} {in_file} | xz -6 -e > {zip_file}\", shell=True, check=True)\n",
    "        # cmd = ['7z', 'a', '-ms=on', f'-mx={compression_level}', zip_file, in_file, base_file]\n",
    "        # print(\" \".join(cmd))\n",
    "        # subprocess.run(cmd)  #, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "    return os.path.getsize(zip_file)\n",
    "    \n",
    "\n",
    "def analyze_compression(task, holdout):\n",
    "    \"\"\"Analyze compression metrics for a single task file\"\"\"\n",
    "    # Get raw size directly from filesystem\n",
    "    task_file = os.path.join(data_dir, task + \".dat\")\n",
    "    raw_size = os.path.getsize(task_file)\n",
    "    \n",
    "    # Get compressed sizes\n",
    "    compressed_size = get_zip_size(task)\n",
    "    holdout_size = get_zip_size(holdout)\n",
    "    mutual_size = get_zip_size(task, base=holdout)\n",
    "    \n",
    "    # Calculate mutual compressed size (additional size beyond holdout)\n",
    "    mutual_compressed_size = mutual_size - holdout_size\n",
    "    \n",
    "    # Calculate ratios\n",
    "    compression_ratio = compressed_size / raw_size\n",
    "    mutual_compression_ratio = mutual_compressed_size / raw_size\n",
    "    \n",
    "    return {\n",
    "        'raw_size': raw_size / MB,\n",
    "        'compressed_size': compressed_size / MB,\n",
    "        'mutual_compressed_size': mutual_compressed_size / MB,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'mutual_compression_ratio': mutual_compression_ratio\n",
    "    }\n",
    "\n",
    "# improvement:\n",
    "# shutil.copy(holdout_zip, mutual_zip)\n",
    "# subprocess.run(['zip', '-9', mutual_zip, task_file], \n",
    "\n",
    "results = {}\n",
    "for task in tasks[:-1]:  # there is no \"Full\" task\n",
    "    out = results[task] = analyze_compression(task, \"holdout\")\n",
    "    print(out)\n",
    "    \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "\n",
    "# Round ratios\n",
    "df['compression_ratio'] = df['compression_ratio'].round(3)\n",
    "df['mutual_compression_ratio'] = df['mutual_compression_ratio'].round(3)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990eec7-2c41-4b19-9be7-0f1d2b2c579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd7f049-906e-4277-a7c9-730345076266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "for i in tqdm(range(20), total=20):\n",
    "    time.sleep(.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e706a4f-8a8a-4203-b2ec-f407ad76d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ee854-7043-41d6-99aa-266839ca22e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded59a2-8da0-4310-b303-f298d7b9c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(fname):\n",
    "    parquet_file = pq.ParquetFile(fname)\n",
    "    metadata = parquet_file.metadata\n",
    "\n",
    "    compressed = 0\n",
    "    raw = 0\n",
    "    # Get total size and rows\n",
    "    total_rows = metadata.num_rows\n",
    "    num_row_groups = metadata.num_row_groups\n",
    "    \n",
    "    # Print overall statistics\n",
    "    # print(f\"Total rows: {total_rows:,}\")\n",
    "    # print(f\"Number of row groups: {num_row_groups}\")\n",
    "    # Get details for each row group\n",
    "    for i in range(num_row_groups):\n",
    "        row_group = metadata.row_group(i)\n",
    "        for j in range(row_group.num_columns):\n",
    "            col = row_group.column(j)\n",
    "            compressed += col.total_compressed_size\n",
    "            raw += col.total_uncompressed_size\n",
    "\n",
    "    return compressed, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0c029-8948-411e-9ae3-f9d03ab235c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, hf_hub_download\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Create directories if they don't exist\n",
    "zip_dir = \"dataset_zips\"\n",
    "os.makedirs(zip_dir, exist_ok=True)\n",
    "\n",
    "# Initialize Hugging Face API\n",
    "api = HfApi()\n",
    "datasets = api.list_datasets(search=\"timaeus/pile-\")\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_name = dataset.id   \n",
    "    task = dataset_name.split(\"pile-\")[1]\n",
    "    if task not in tasks:\n",
    "        # Only get the files\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # Get list of files\n",
    "    files = api.list_repo_files(dataset_name, repo_type=\"dataset\")\n",
    "    files = [f for f in files if not (f.endswith('.json') or f.endswith('.md') or f == '.gitattributes')]\n",
    "    if not(files):\n",
    "        continue\n",
    "\n",
    "    infos = api.get_paths_info(repo_id=dataset_name, paths=files, repo_type=\"dataset\")\n",
    "    total_size = sum([f.size for f in infos])\n",
    "    \n",
    "    # Download files until we reach target size or run out of files\n",
    "    downloaded_files = []\n",
    "    current_size = 0\n",
    "    for file in files:\n",
    "        local_file = hf_hub_download(\n",
    "            repo_id=dataset_name,\n",
    "            filename=file,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        file_size = os.path.getsize(local_file)\n",
    "        if file_size > 1024:  # Skip tiny files\n",
    "            downloaded_files.append(local_file)\n",
    "            current_size += file_size\n",
    "            #print(f\"Added {file}: {file_size / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        if current_size >= TARGET_SAMPLE_SIZE:\n",
    "            break    \n",
    "   \n",
    "\n",
    "    snappy = 0\n",
    "    zippy = 0\n",
    "    raw = 0\n",
    "    \n",
    "    for f in downloaded_files:\n",
    "        # print(f\"Loading {f}\")\n",
    "\n",
    "        c, r = get_size(f)\n",
    "        snappy += c\n",
    "        raw += r\n",
    "        \n",
    "        f2 = f[:-4] + \"_zip.parquet\"\n",
    "\n",
    "        if not os.path.exists(f2):\n",
    "            print(\"Recompressing\")\n",
    "            table = pq.read_table(f)\n",
    "            pq.write_table(table, f2, compression=\"gzip\", compression_level=8)  # max is 9\n",
    "            del table\n",
    "\n",
    "        c2, r2 = get_size(f2)\n",
    "        zippy += c2\n",
    "        print(r, r2)\n",
    "       \n",
    "        \n",
    "    \n",
    "    item = {\n",
    "        'task': task,\n",
    "        'uncompressed_size_mb': raw / (1024*1024),\n",
    "        'compressed_snappy_mb': snappy / (1024*1024),\n",
    "        'compressed_zip_mb': zippy / (1024*1024),\n",
    "        'zip_ratio': zippy / raw,\n",
    "        'snappy_ratio': snappy / raw,\n",
    "        'files_analyzed': len(downloaded_files),\n",
    "        'total_files': len(files),\n",
    "        'snappy_size': total_size,        \n",
    "    }\n",
    "    results.append(item)\n",
    "    \n",
    "    print(dataset_name)\n",
    "    print(f\"snappy ratio: {snappy / raw:.0%}\")\n",
    "    print(f\"zipped ratio: {zippy / raw:.0%}\")\n",
    "    print(f\"snappy size: {total_size/1024/1024:.1f}MB\")\n",
    "\n",
    "\n",
    "# Convert results to DataFrame\n",
    "sizes = pd.DataFrame(results).set_index(\"task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65b456-9717-4539-9cdb-010a00c24a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = api.list_datasets(search=\"timaeus/pile-\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e17830-d491-487f-b003-3cc2bb52d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps i'm measuring the wrong thing here - its more like if you train on everything, how much to memorise this specific dataset?\n",
    "# which is like making a zip of half the data, then looking at adding the new dataset to the mix\n",
    "# mutual information.. pointing the same direction\n",
    "# like resolution of compressibility\n",
    "result = sizes.join(coefs)\n",
    "\n",
    "fig = go.Figure()\n",
    "# xcol = \"compressed_zip_mb\"\n",
    "xcol = \"zip_ratio\"\n",
    "ycol = \"r\"\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=result[xcol], y=result[ycol], mode='markers',\n",
    "               customdata=result.index,\n",
    "               hovertemplate='%{customdata}<br>' + xcol + ': %{x}<br>' + ycol + ': %{y}<extra></extra>'\n",
    "              )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=xcol,\n",
    "    yaxis_title=ycol,\n",
    "    #xaxis_type=\"log\",  # log scale for x axis\n",
    "    #yaxis_type=\"log\"   # log scale for y axis\n",
    "    width=400,\n",
    "    height=400,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f5ae6-e60c-4433-b8b2-48ba4aecf259",
   "metadata": {},
   "source": [
    "## Appendix: Loss Scaling\n",
    "\n",
    "A standard \"scaling law\" for loss that I've seen (eg. in Hoffmann et al. 2022 Training compute-optimal large language models, and Choshen et al. A Hitchiker's guide to scaling law estimation) is of the form:\n",
    "\n",
    "#### Across model size\n",
    "$ L - L_0 = \\frac{A}{P^{\\alpha}} + \\frac{B}{T^{\\alpha}} $\n",
    "Note this 5 parameter model is wrt size and time - it is a 3 parameter model wrt time.\n",
    "\n",
    "#### Fixed model size\n",
    "$ (L - L_0) = c T^{-r}$, and is in terms of step (T) rather than LLC, and it assumes $T_0 = 0$\n",
    "\n",
    "T = steps gets a bit handwavy if the learning rate changes, but Choshen et al observed there is still some value to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3edd3-7873-4234-a92f-1083868a9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = go.Figure()\n",
    "xlabel = \"step\"\n",
    "ylabel = \"loss\"\n",
    "\n",
    "def L(T, params):\n",
    "    L0 = np.exp(params[0])  # log-parameters required for convergence\n",
    "    c = np.exp(params[1])\n",
    "    r = params[2]\n",
    "    return np.maximum(1e-8, L0 + c / (T ** r))\n",
    "\n",
    "# Record the per-task parameters\n",
    "params_list = []\n",
    "\n",
    "# Iterate through corresponding pairs of columns\n",
    "for task, col in zip(tasks, colors):\n",
    "\n",
    "    # plot as we go    \n",
    "    fig.add_trace(        \n",
    "        go.Scatter(\n",
    "            x=df_loss_trim.index.values,\n",
    "            y=df_loss_trim[task].values,\n",
    "            mode='markers', # 'lines+markers',\n",
    "            name=task,\n",
    "            line=dict(color=col),\n",
    "            marker=dict(color=col),\n",
    "            hovertemplate=(\n",
    "                f\"Task: {task}<br>\" +\n",
    "                xlabel + \": %{x}<br>\" +\n",
    "                ylabel + \": %{y}<br>\" +\n",
    "                \"<extra></extra>\"\n",
    "            ),\n",
    "            customdata=df_loss.index,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Fit a power law\n",
    "    x = df_loss_trim.index.values.astype(float)\n",
    "    y = df_loss_trim[task].values\n",
    "    \n",
    "    def loss(params):\n",
    "        est = L(x, params)\n",
    "        #return np.sum((est - y)**2)\n",
    "        return np.sum((np.log(est) - np.log(y))**2)\n",
    "\n",
    "    # heuristic for optimisation that's not crazy\n",
    "    par0 = [1., 6., 1.]\n",
    "    res = minimize(loss, par0, method=\"L-BFGS-B\", jac=grad(loss))\n",
    "    par = res.x\n",
    "    \n",
    "    L0, c, r = par\n",
    "    params_list.append({\n",
    "        'logL0': L0,\n",
    "        'logc': c,\n",
    "        'r': r,\n",
    "    })\n",
    "\n",
    "    xp = np.exp(np.linspace(np.log(x.min()), np.log(x.max()), 100))\n",
    "    f = L(xp, par)\n",
    "    \n",
    "    fig.add_trace(        \n",
    "        go.Scatter(\n",
    "            x=xp,\n",
    "            y=f,\n",
    "            mode='lines',\n",
    "            name=task + \"power law\",\n",
    "            line=dict(color=col),\n",
    "            hovertemplate=(\n",
    "                f\"Task: {task}<br>\" +\n",
    "                f\"L0: {L0}<br>\" +\n",
    "                f\"c: {c}<br>\" +\n",
    "                f\"r: {r}<br>\" +\n",
    "                \"<extra></extra>\"\n",
    "            ),\n",
    "            customdata=df_loss_trim.index,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    xaxis_title=xlabel,\n",
    "    yaxis_title=ylabel,\n",
    "    xaxis_type=\"log\",\n",
    "    yaxis_type=\"log\",\n",
    "    title=\"Loss scaling\",\n",
    "    showlegend=True,\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "results = pd.DataFrame(params_list)\n",
    "results.index = tasks\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4976d-7d54-4e0c-819c-b0b10113e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First, collect a bunch of samples from the \"full\" dataset\n",
    "# # It might also be interesting to know the dictionary size...\n",
    "# dataset_name = \"timaeus/dsir-pile-1m-2\"\n",
    "# key = \"contents\"\n",
    "# dataset = load_dataset(dataset_name, token=token)[\"train\"]  # are they already shuffled?\n",
    "# count = 0\n",
    "# target = 1000\n",
    "# data_dir = \"datasets/samples\"\n",
    "# # samples 21 thru 24 are consecutive, telling me that its not shuffled at all....\n",
    "\n",
    "# os.makedirs(data_dir, exist_ok=True)\n",
    "# for batch in tqdm(dataset.iter(batch_size=500), total=2000):\n",
    "#     for text in batch[key]:\n",
    "#         with open(f\"{data_dir}/sample_{count:03d}\", \"w\") as f:\n",
    "#             f.write(text)\n",
    "#         count += 1\n",
    "#         break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
