{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2a9edd-321b-4353-8417-1ccf232d06ff",
   "metadata": {},
   "source": [
    "# EVAL-158\n",
    "## Does predicting loss of later checkpoints from LLC reliably improve over the prediction from checkpoint index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9607b-f3d9-4429-8f39-b325fd5b6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import fitting as fit\n",
    "import data_utils as dat\n",
    "import vis\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "#from plotly.subplots import make_subplots\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb24286-4d7a-4e76-9b68-8f1e8e65cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some example data\n",
    "# At 160m, the regular loss vs time seems better\n",
    "mcode = \"1b\"  #\"160m\"  # \"410m-dense\"\n",
    "msize = mcode.split(\"-\")[0]\n",
    "df_llc, df_loss = dat.load_dfs(mcode, data_path=\"data\")\n",
    "#tasks = [\"github\", \"stackexchange\", \"arxiv\", \"pile-cc\", \"pubmed_abstracts\"]  # and full?\n",
    "tasks = df_llc.columns\n",
    "step_start = 2000  # are we cropping too early?\n",
    "step_cutoff = 20000   # How much do we get to observe? about 10% of training?\n",
    "step_end = 200000 # 80000 end of reigeme, or end of training?\n",
    "\n",
    "scale = 1000.  # rescale steps for the time-fits only\n",
    "colors = vis.assign_cols(tasks)  #df_llc.columns)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6191dd8-605a-41a3-aa32-72bdb425c9d8",
   "metadata": {},
   "source": [
    "## Loss vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fef71e-5173-4c99-907b-ae56b7b2cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for loss\n",
    "fig = go.Figure()\n",
    "\n",
    "for task in tasks:\n",
    "    llc, loss, steps = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "\n",
    "    # Prepare holdout data (fixed cutoff for now)\n",
    "    trace = dat.Trace(steps/scale, loss, steps)\n",
    "    train, test = dat.split(trace, step_cutoff)\n",
    "    loss_model = fit.min_fit(train.x, train.y, fit.OffsetPowerLaw2)\n",
    "\n",
    "\n",
    "    # Plot\n",
    "    color = colors[task]\n",
    "    color2 = vis.add_color(color)  # show faded holdout data\n",
    "    vis.plot_data(fig, train.x, train.y, train.s, color=color, showlegend=False, size=5, xscale=scale) \n",
    "    vis.plot_data(fig, test.x, test.y, test.s, color=color2, showlegend=False, size=5, xscale=scale) \n",
    "    vis.plot_result(fig, trace.x, loss_model, name=task, xscale=scale, color=color, showlegend=True, res=600)\n",
    "    fig.update_xaxes(title_text=\"Step\", type=\"log\")\n",
    "    fig.update_yaxes(title_text=r\"Loss L\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    showlegend=True,\n",
    "\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d425d1-b608-45b4-b4bf-c2fe7373abd2",
   "metadata": {},
   "source": [
    "## What's going on late training? Its almost like a new exponent\n",
    "## Actually this isn't fitting that well, its just the line is very flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dffbe0-f19b-410e-b44d-dabd1b3d94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_start2 = 12000\n",
    "step_cutoff2 = 40000\n",
    "step_end2 = 200000\n",
    "fig = go.Figure()\n",
    "\n",
    "for task in tasks:\n",
    "    llc, loss, steps = dat.trim_trace(df_llc, df_loss, task, step_start2, step_end2)\n",
    "\n",
    "    # Prepare holdout data (fixed cutoff for now)\n",
    "    trace = dat.Trace(steps/scale, loss, steps)\n",
    "    train, test = dat.split(trace, step_cutoff2)\n",
    "    loss_model = fit.min_fit(train.x, train.y, fit.OffsetLogarithm)  #fit.OffsetPowerLaw2\n",
    "    \n",
    "    \n",
    "    # Evaluate the metrics\n",
    "    y_pred = loss_model.f(test.x)\n",
    "    RMSE = fit.rmse(test.y, y_pred)\n",
    "\n",
    "    # Plot\n",
    "    color = colors[task]\n",
    "    color2 = vis.add_color(color)  # show faded holdout data\n",
    "    vis.plot_data(fig, train.x, train.y, train.s, color=color, showlegend=False, size=5, xscale=scale) \n",
    "    vis.plot_data(fig, test.x, test.y, test.s, color=color2, showlegend=False, size=5, xscale=scale) \n",
    "    vis.plot_result(fig, trace.x, loss_model, name=task, xscale=scale, color=color, showlegend=True, res=600)\n",
    "    fig.update_xaxes(title_text=\"Step\", type=\"log\")\n",
    "    fig.update_yaxes(title_text=r\"Loss L\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    showlegend=True,\n",
    "\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd224b-5f64-4b2c-9ac4-3e7801422758",
   "metadata": {},
   "source": [
    "## LLC vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d00ba6b-f334-4ce7-a668-3df4f025f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically a cut'n'paste job\n",
    "\n",
    "\n",
    "# Candidate functions:\n",
    "functions = [\n",
    "    #Powerlaw needs some help because the initial conditions are very different\n",
    "    (fit.OffsetPowerLaw, dict(par0=[10., -10., .1]), \"Powerlaw\", \"red\"),\n",
    "    (fit.OffsetLogarithm, {}, \"Logarithm\", \"blue\"),\n",
    "]\n",
    "\n",
    "# Make a grid layout\n",
    "titles = []\n",
    "for f in functions:\n",
    "    for s in [\"Fit\", \"Holdout\"]:\n",
    "        titles.append(f\"{f[2]} - {s}\")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for task in tasks:\n",
    "    # Load\n",
    "    llc, loss, steps = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "    trace = dat.Trace(steps/scale, llc, steps)  # llc vs step\n",
    "    train, test = dat.split(trace, step_cutoff)\n",
    "\n",
    "    # Plot the data\n",
    "    color = colors[task]\n",
    "    color2 = vis.add_color(color)  # for heldout data\n",
    "    vis.plot_data(fig, train, color=color, showlegend=False, size=5, xscale=scale) \n",
    "    vis.plot_data(fig, test, color=color2, showlegend=False, size=5, xscale=scale) \n",
    "        \n",
    "    \n",
    "    for f_ind, (function, args, fname, fcol) in enumerate(functions):   \n",
    "    \n",
    "        # Fit\n",
    "        llc_model = fit.min_fit(train.x, train.y, function, **args)\n",
    "\n",
    "        # Plot the fit\n",
    "        vis.plot_result(fig, trace.x, llc_model, name=f\"{task}-{fname}\",\n",
    "                        xscale=scale, color=fcol, showlegend=True)\n",
    "        fig.update_xaxes(title_text=\"Step\", type=\"log\")\n",
    "        fig.update_yaxes(title_text=\"Scaled LLC\")\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    showlegend=True,\n",
    "\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66797bf-4a91-453b-bc4f-0ca33d3ca28c",
   "metadata": {},
   "source": [
    "Thoughts - both pretty good, I kinda like the logarithm\n",
    "* the LLC curves up, and logarithm predicts higher therefore it holds out a tiny bit longer\n",
    "* logarithm is way simpler\n",
    "* Both **fall apart** around step 80k (the end of the analysis interval)\n",
    "\n",
    "## LLC vs loss\n",
    "\n",
    "we can use the function result.model.inverse(x, result.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d97e7-fa5b-462d-b67e-172804873636",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for task in tasks:\n",
    "    llc, loss, steps = trace = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "    train, test = dat.split(trace, step_cutoff)\n",
    "    loss_model = fit.min_fit(train.x, train.y, fit.OffsetPowerLaw)\n",
    "    # y_pred = loss_model.f(test.x)\n",
    "    # RMSE = fit.rmse(test.y, y_pred)\n",
    "\n",
    "    # Plot\n",
    "    color = colors[task]\n",
    "    color2 = vis.add_color(color)  # show faded holdout data\n",
    "    vis.plot_data(fig, train, color=color, showlegend=False, size=5, xscale=scale) \n",
    "    vis.plot_data(fig, test, color=color2, showlegend=False, size=5, xscale=scale) \n",
    "    vis.plot_result(fig, trace.x, loss_model, name=task, xscale=scale, color=color, showlegend=True, res=600)\n",
    "    fig.update_xaxes(title_text=\"Step\", type=\"log\")\n",
    "    fig.update_yaxes(title_text=r\"Loss L\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    showlegend=True,\n",
    "\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f19ef5-058c-49b8-b043-f1b58cfa4952",
   "metadata": {},
   "source": [
    "# Comparing loss(time) vs loss(llc(time))\n",
    "- Comparing L(T), L(LLC(T))\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9159b3-90ed-4e90-b3dd-ed695732da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(x, y, at):\n",
    "    out = []\n",
    "    for i in at:\n",
    "        idx = np.searchsorted(x, i)\n",
    "        if idx >= len(x):\n",
    "            idx = len(x)-1\n",
    "        if x[idx] == i:\n",
    "            out.append(y[idx])\n",
    "        else:\n",
    "            out.append(np.nan)\n",
    "        \n",
    "    return np.array(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b97455-c177-478a-b3b0-348b33b1210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_at = np.array([50000, 70000, 143000])\n",
    "eval_scaled = eval_at / scale\n",
    "columns = []\n",
    "row_names = [\"L(T)\", \"L*(T)\", \"L(LLC(T))\"]\n",
    "col_names = []\n",
    "n_col = len(eval_at) * len(tasks)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "end = len(tasks) - 1\n",
    "table = np.zeros((len(row_names), n_col))\n",
    "col = 0\n",
    "\n",
    "for i, task in enumerate(tasks):\n",
    "    color = colors[task]\n",
    "    color2 = vis.add_color(color)  # show faded holdout data\n",
    "    main = dat.trim_trace(df_llc, df_loss, task, step_start, step_end)\n",
    "\n",
    "    # Loss vs time\n",
    "    loss_t = dat.Trace(main.s / scale, main.y, main.s)  # loss vs time\n",
    "    train, test = dat.split(loss_t, step_cutoff)\n",
    "    loss_model = fit.min_fit(train.x, train.y, fit.OffsetPowerLaw)\n",
    "    \n",
    "    # Fit with same functional form as the composition\n",
    "    comp_model = fit.min_fit(train.x, train.y, fit.Modron)\n",
    "\n",
    "    # Plot loss vs time\n",
    "    \n",
    "    vis.plot_data(fig, train, color=color, showlegend=True, size=5, xscale=scale, name=task) \n",
    "    vis.plot_data(fig, test, color=color2, showlegend=False, size=5, xscale=scale) \n",
    "    \n",
    "    # Fit loss vs LLC\n",
    "    loss_llc = main\n",
    "    train, _ = dat.split(loss_llc, step_cutoff)\n",
    "    loss_ = fit.min_fit(train.x, train.y, fit.OffsetPowerLaw)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fit LLC vs time:\n",
    "    llc_t = dat.Trace(main.s / scale, main.x, main.s)  # llc vs time\n",
    "    train, _ = dat.split(llc_t, step_cutoff)\n",
    "    llc_ = fit.min_fit(train.x, train.y, fit.OffsetLogarithm)  # loss vs LLC\n",
    "\n",
    "    # Plot loss(llc(time)):\n",
    "\n",
    "    def composed(x):\n",
    "        return loss_.f(llc_.f(x))\n",
    "\n",
    "\n",
    "    vis.plot_result(fig, loss_t.x, loss_model, name=\"L(T)\", xscale=scale, color=\"blue\", showlegend=(i==end), res=600)\n",
    "    vis.plot_result(fig, loss_t.x, comp_model, name=\"comp(T)\", xscale=scale, color=\"green\", showlegend=(i==end), res=600)    \n",
    "    vis.plot_result(fig, llc_t.x, composed, name=\"L(LLC(t))\", xscale=scale, color=\"red\", showlegend=(i==end), res=600)\n",
    "\n",
    "    # Collate prediction errors at certain points\n",
    "    truth = get_y(main.s, main.y, eval_at)\n",
    "    table[0, col:col+3] = loss_model.f(eval_scaled) - truth\n",
    "    table[1, col:col+3] = comp_model.f(eval_scaled) - truth\n",
    "    table[2, col:col+3] = composed(eval_scaled) - truth\n",
    "    col += 3\n",
    "    for a in eval_at:\n",
    "        col_names.append(f\"{a//1000}k@{task}\")\n",
    "\n",
    "\n",
    "    \n",
    "fig.update_xaxes(title_text=\"Step\", type=\"log\")\n",
    "fig.update_yaxes(title_text=r\"Loss L\")\n",
    "fig.update_layout(\n",
    "    title=\"Loss vs Time\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    showlegend=True,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b71882-b11e-4821-bc31-84254a536961",
   "metadata": {},
   "source": [
    "# Conclusion - seems better than a simple powerlaw....\n",
    "## At least until ~80k when the model for LLC(t) falls apart\n",
    "## but does seeing more data actually help?(and has more parameters?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a05a46-f3fd-43b7-9f80-94463ddc0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulate prediction error at 70k and 143k\n",
    "result = pd.DataFrame(\n",
    "    table,\n",
    "    index=row_names,\n",
    "    columns=col_names,\n",
    ")\n",
    "result = result.sort_index(axis=1)\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "def highlight_min_magnitude(row):\n",
    "    # Find the index of the minimum absolute value in the row\n",
    "    min_idx = np.abs(row).argmin()\n",
    "    \n",
    "    # Create a list of empty strings the same length as the row\n",
    "    result = ['' for _ in range(len(row))]\n",
    "    \n",
    "    # Add bold HTML tag to the smallest magnitude value\n",
    "    result[min_idx] = 'font-weight: bold'\n",
    "    \n",
    "    return result\n",
    "\n",
    "def highlight_min_magnitude_in_columns(col):\n",
    "    # Find the index of the minimum absolute value in the column\n",
    "    min_idx = np.abs(col).argmin()\n",
    "    \n",
    "    # Create a list of empty strings the same length as the column\n",
    "    result = ['' for _ in range(len(col))]\n",
    "    \n",
    "    # Add bold HTML tag to the smallest magnitude value\n",
    "    result[min_idx] = 'font-weight: bold'\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "columns = result.columns\n",
    "for a in eval_at:\n",
    "    start = f\"{a//1000}k\"\n",
    "    ex = [c for c in columns if c.startswith(start)]\n",
    "    extract = result[ex]\n",
    "    styled = extract.style.apply(highlight_min_magnitude_in_columns, axis=0)\n",
    "    display(styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b70214-f645-4658-b826-5b1b77d82a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interestingly more parameters isn't overfitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316dec64-300f-4df4-873c-04fc5f9085ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
