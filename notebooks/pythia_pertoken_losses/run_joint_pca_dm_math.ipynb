{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Enable automatic module reloading (great for development)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add the directory with your visualization module to the path\n",
    "sys.path.append(\"/Users/liam/quests/lsoc-psych/datasets\")\n",
    "\n",
    "# Import the visualization functions\n",
    "from joint_pca import load_trajectory_data, TrajectoryPCA, load_token_mapping_dm_math, load_dual_trajectory_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1636 answer columns\n",
      "Loading data for model sizes: ['6.9b', '160m', '31m', '2.8b', '1.4b', '70m', '14m', '410m', '1b']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading trajectory data: 100%|██████████| 9/9 [00:01<00:00,  5.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from joint_pca import load_dual_trajectory_data\n",
    "experiment_name = \"EXP000\"\n",
    "dataset_name = \"dm_mathematics\"\n",
    "num_contexts = None\n",
    "# model_sizes = ['6.9b', '160m', '31m', '2.8b', '1.4b', '70m', '410m', '1b']\n",
    "model_sizes = None\n",
    "trajectory_data = load_dual_trajectory_data(experiment_name,\n",
    "                                            dataset_name,\n",
    "                                            num_contexts=num_contexts,\n",
    "                                            model_sizes=model_sizes,\n",
    "                                            answer_only=True)\n",
    "token_mapping, token_data = load_token_mapping_dm_math(experiment_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot columns: 1565\n",
      "Zero-shot columns: 1565\n",
      "Combined columns: 3130\n",
      "Combined matrix shape: (1386, 3130)\n",
      "Regular PCA: Top 10 components explain 94.34% of variance\n",
      "Individual explained variance: [0.78143033 0.10426991 0.01696341 0.01144386 0.00912636 0.00575838\n",
      " 0.00419508 0.00397459 0.00331909 0.00289929]\n"
     ]
    }
   ],
   "source": [
    "from joint_pca import DualTrajectoryPCA\n",
    "\n",
    "step_range = [None, None]\n",
    "n_components = 10\n",
    "n_sparse_components = 0\n",
    "\n",
    "dual_pca = DualTrajectoryPCA(trajectory_data,\n",
    "                             step_range = step_range,\n",
    "                             n_components=n_components,\n",
    "                             n_sparse_components=n_sparse_components,\n",
    "                             scale=False,\n",
    "                             sparse_pca_params={'alpha':5},\n",
    "                             run_at_init=True,\n",
    "                             dataset_name=dataset_name,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REGULAR PCA COMPONENTS - COSINE SIMILARITY WITH SHOT TYPES\n",
      "================================================================================\n",
      "Component  Few-Shot Similarity  Zero-Shot Similarity  Bias (Few - Zero)\n",
      "      PC1               0.7057                0.6784             0.0272\n",
      "      PC2               0.1148                0.0293             0.0855\n",
      "      PC3               0.0370                0.0144             0.0227\n",
      "      PC4               0.0633               -0.1328             0.1960\n",
      "      PC5              -0.0939                0.1574            -0.2512\n",
      "      PC6               0.0190                0.0161             0.0029\n",
      "      PC7              -0.0546                0.0640            -0.1187\n",
      "      PC8              -0.0186                0.0622            -0.0808\n",
      "      PC9               0.0713               -0.0642             0.1354\n",
      "     PC10              -0.0232                0.0440            -0.0672\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from joint_pca import compute_pc_shot_cosine_similarity, print_pc_shot_cosine_table\n",
    "\n",
    "print_pc_shot_cosine_table(dual_pca, regular=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joint_pca import TrajectoryPlotter\n",
    "\n",
    "plotter = TrajectoryPlotter(dual_pca)\n",
    "fig = plotter.plot_pcs_over_time()\n",
    "raw_filename = plotter._get_filename()\n",
    "filename = raw_filename + \"_pcs_over_time\"\n",
    "fig.write_image(filename + \".png\", scale=2)\n",
    "fig.write_html(filename + \".html\")\n",
    "fig.write_image(filename + \".pdf\")\n",
    "\n",
    "for comp in range(1, 10+1):\n",
    "    # comp with one leading zero in name\n",
    "    filename = raw_filename + f\"_loaded_features_comp{comp:02d}\"\n",
    "    fig = plotter.plot_top_loaded_features_for_component(component=comp, token_mapping=token_mapping, token_data=token_data)\n",
    "    fig.write_image(filename + \".png\", scale=2)\n",
    "    fig.write_html(filename + \".html\")\n",
    "    fig.write_image(filename + \".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot columns: 1565\n",
      "Zero-shot columns: 1565\n",
      "Combined columns: 3130\n",
      "Combined matrix shape: (1386, 3130)\n",
      "Using full matrix for sparse PCA (no regular PCA performed): (1386, 3130)\n",
      "Sparsity of components (fraction of zero values): [0.22140575079872205, 0.6492012779552716, 0.4514376996805112, 0.7517571884984026, 0.28402555910543126, 0.5402555910543131, 0.8916932907348243, 0.8980830670926517, 0.6188498402555911, 0.6124600638977635]\n"
     ]
    }
   ],
   "source": [
    "from joint_pca import DualTrajectoryPCA\n",
    "\n",
    "step_range = [None, None]\n",
    "n_components = 0\n",
    "n_sparse_components = 10\n",
    "\n",
    "dual_pca = DualTrajectoryPCA(trajectory_data,\n",
    "                             step_range = step_range,\n",
    "                             n_components=n_components,\n",
    "                             n_sparse_components=n_sparse_components,\n",
    "                             scale=False,\n",
    "                             sparse_pca_params={'alpha':5},\n",
    "                             run_at_init=True,\n",
    "                             dataset_name=dataset_name,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARSE PCA COMPONENTS - COSINE SIMILARITY WITH SHOT TYPES\n",
      "================================================================================\n",
      "Component  Few-Shot Similarity  Zero-Shot Similarity  Bias (Few - Zero)\n",
      "     SPC1               0.5561                0.5159             0.0402\n",
      "     SPC2               0.2367                0.1567             0.0800\n",
      "     SPC3               0.2910                0.4258            -0.1349\n",
      "     SPC4               0.0669                0.2768            -0.2099\n",
      "     SPC5               0.5350                0.3796             0.1554\n",
      "     SPC6               0.3003                0.3345            -0.0342\n",
      "     SPC7               0.0449               -0.0236             0.0685\n",
      "     SPC8               0.0164               -0.0153             0.0317\n",
      "     SPC9               0.2911                0.2887             0.0024\n",
      "    SPC10               0.2182                0.3378            -0.1196\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from joint_pca import compute_pc_shot_cosine_similarity, print_pc_shot_cosine_table\n",
    "\n",
    "print_pc_shot_cosine_table(dual_pca, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joint_pca import TrajectoryPlotter\n",
    "\n",
    "plotter = TrajectoryPlotter(dual_pca)\n",
    "fig = plotter.plot_pcs_over_time()\n",
    "raw_filename = plotter._get_filename() \n",
    "filename = raw_filename + \"_pcs_over_time\"\n",
    "fig.write_image(filename + \".png\", scale=2)\n",
    "fig.write_html(filename + \".html\")\n",
    "fig.write_image(filename + \".pdf\")\n",
    "\n",
    "for comp in range(1, 10+1):\n",
    "    # comp with one leading zero in name\n",
    "    filename = raw_filename + f\"_loaded_features_comp{comp:02d}\"\n",
    "    fig = plotter.plot_top_loaded_features_for_component(component=comp, token_mapping=token_mapping, token_data=token_data)\n",
    "    fig.write_image(filename + \".png\", scale=2)\n",
    "    fig.write_html(filename + \".html\")\n",
    "    fig.write_image(filename + \".pdf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot columns: 1565\n",
      "Zero-shot columns: 1565\n",
      "Combined columns: 3130\n",
      "Combined matrix shape: (612, 3130)\n",
      "Using full matrix for sparse PCA (no regular PCA performed): (612, 3130)\n",
      "Sparsity of components (fraction of zero values): [0.7757188498402556, 0.6741214057507987, 0.8696485623003195, 0.789776357827476, 0.9191693290734824, 0.9482428115015974, 0.9568690095846646, 0.7252396166134185, 0.9217252396166135, 0.6258785942492013]\n"
     ]
    }
   ],
   "source": [
    "from joint_pca import DualTrajectoryPCA\n",
    "\n",
    "step_range = [3000, 70000]\n",
    "n_components = 0\n",
    "n_sparse_components = 10\n",
    "\n",
    "dual_pca = DualTrajectoryPCA(trajectory_data,\n",
    "                             step_range = step_range,\n",
    "                             n_components=n_components,\n",
    "                             n_sparse_components=n_sparse_components,\n",
    "                             scale=False,\n",
    "                             sparse_pca_params={'alpha':5},\n",
    "                             run_at_init=True,\n",
    "                             dataset_name=dataset_name,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARSE PCA COMPONENTS - COSINE SIMILARITY WITH SHOT TYPES\n",
      "================================================================================\n",
      "Component  Few-Shot Similarity  Zero-Shot Similarity  Bias (Few - Zero)\n",
      "     SPC1               0.1420                0.1100             0.0319\n",
      "     SPC2               0.2728                0.2198             0.0530\n",
      "     SPC3               0.0328                0.2542            -0.2214\n",
      "     SPC4               0.1672                0.1660             0.0011\n",
      "     SPC5               0.0226                0.2687            -0.2461\n",
      "     SPC6              -0.0219                0.0522            -0.0741\n",
      "     SPC7               0.0238               -0.0057             0.0295\n",
      "     SPC8               0.2636                0.1760             0.0876\n",
      "     SPC9               0.0172                0.1353            -0.1180\n",
      "    SPC10               0.2285                0.3486            -0.1201\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from joint_pca import compute_pc_shot_cosine_similarity, print_pc_shot_cosine_table\n",
    "\n",
    "print_pc_shot_cosine_table(dual_pca, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joint_pca import TrajectoryPlotter\n",
    "\n",
    "plotter = TrajectoryPlotter(dual_pca)\n",
    "fig = plotter.plot_pcs_over_time()\n",
    "raw_filename = plotter._get_filename()\n",
    "filename = raw_filename + \"_pcs_over_time\"\n",
    "fig.write_image(filename + \".png\", scale=2)\n",
    "fig.write_html(filename + \".html\")\n",
    "fig.write_image(filename + \".pdf\")\n",
    "\n",
    "for comp in range(1, 10+1):\n",
    "    # comp with one leading zero in name\n",
    "    filename = raw_filename + f\"_loaded_features_comp{comp:02d}\"\n",
    "    fig = plotter.plot_top_loaded_features_for_component(component=comp, token_mapping=token_mapping, token_data=token_data)\n",
    "    fig.write_image(filename + \".png\", scale=2)\n",
    "    fig.write_html(filename + \".html\")\n",
    "    fig.write_image(filename + \".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liam's explanation\n",
    "\n",
    "## Pertoken losses\n",
    "\n",
    "For each Pile subset $P$ (aside from `dm_mathematics` which I'll write about below), we constructed a dataset $D_P$ of $K=512$ contexts $\\{S_k^P\\}_{k=1}^K$, where each $|S_k| \\leq 512$ in token length (some datasets, like `enron_emails`, have small contexts, but most max out at 512 tokens). Each $S_k$ is randomly sampled from a context from Timaeus' pile-subsets (in particular: we first randomly sample a context, and then randomly sample the token slice from within that context, so that the contexts didn't just represent text beginnings).\n",
    "\n",
    "Given a dataset $D_P$, for each Pythia model size $M_t$ in the suite $mathcal{M} = [14m, ..., 6.9b]$ at training checkpoint $t$, we evaluate the next-token prediction loss on each context $S_K^P$, which I will denote $\\ell_{M_t, S_K^P}$. (The 0th token is always padded with $\\ell=0$, since pythia doesn't use a `<BOS>` token). Thus, for each model $M \\in \\mathcal{M}$ and each dataset $D_P$, we construct a trajectory pertoken loss matrix $L_{M, P}$ of size `(num_checkpoints, total_tokens )` where `total_tokens = \\sum_k |S_k|` with each $\\ell_{M_t, S_K^P}$ in the appropriate place. \n",
    "\n",
    "`dm_mathematics` is a special dataset, where we constructed 560 problems $x$ with 10 contexts per category (where a category is a module and a template, in the language of that paper - e.g. `algebra - linear_1d`). Each problem has a solution, which also follows after the `\"\\n\"` token, which allows for more fine-grained analysis. This allows us to construct two datasets:\n",
    "- $D_{DM, zero}$ is the dataset of 560 contexts such that each context is one problem with one answer (thus zero-shot asking the question). \n",
    "- $D_{DM, few}$ is the dataset of 56 contexts such that each context represents a _category_, where all problems from a given category are concatenated (up to a certain max-token-threshold [I think 512]) to form a few-shot context. The problems are separated by a `\"\\n\"` token, which we do not track the pertoken loss of. \n",
    "\n",
    "This means that $L_{M, D_{DM, zero}}$ and $L_{M,D_{DM, few}}$ have the exact same dimensions, where each row and column matches; the only thing that is different is the loss calculated upon either the few-shot or zero-shot strategy. \n",
    "\n",
    "## Pertoken loss HTMLs\n",
    "Let $L_{M, P'}$ be the pertoken loss matrix above shortened to only contain the data of contexts $D_P' = \\{S_k\\}_{k=1}^50$, which we restrict for computational efficiency (otherwise the matrices are too big to dsiplay/do PCA on). Then for non-`dm_mathematics` Pile subsets: \n",
    "- The \"raw loss\" pane of the pertoken loss HTMLs is just displaying the pure data in $L_{M, P'}$.\n",
    "- Given a subset of checkpoints $T \\subseteq \\mathcal{T}$ as selected by the user, the \"stepwise difference\" calculates $L_{M_t, P'} - L_{M_{t-1}, P'}$ for each index of $T$. (Therefore each row displays the difference between that row and the one before it, e.g. `step1000 - step512`).\n",
    "- The \"model difference\" pane displays $L_{M_2, P'} - L_{M_1, P'}$ for two models $M_1, M_2 \\in \\mathcal{M}$. \n",
    "\n",
    "For `dm_mathematics` specifically, all of the above applies, plus: \n",
    "- The raw loss is either showing $L_{M, D_{DM, zero}}$ or $L_{M,D_{DM, few}}$\n",
    "- The prompting difference is showing $L_{M,D_{DM, few}} - L_{M, D_{DM, zero}}$. \n",
    "\n",
    "## PCA \n",
    "\n",
    "As in the Essential Dynamics project, for each dataset $D_P'$ we construct a large matrix $L_{\\mathcal{M}, P}$ which vertically concatenates $[L_{M_1, P'}, \\dots, L_{M_{max}, P'}]^{\\top}$ to create a joint trajectory matrix of size `(num_models*num_checkpoints, total_num_tokens)`. We then apply some different methods: \n",
    "- Full PCA on $L_{\\mathcal{M}, P}$, extracting 10 components. \n",
    "- Sparse PCA on $L_{\\mathcal{M}, P}$, extracting 10 components. Here I used \n",
    "\n",
    "```\n",
    "default_sparse_params = {\n",
    "                'alpha': 5.0,  # L1 penalty parameter\n",
    "                'ridge_alpha': 0.01,  # Ridge penalty parameter\n",
    "                'max_iter': 1000,\n",
    "                'tol': 1e-6,\n",
    "                'random_state': 42}\n",
    "```\n",
    ", but in one experiment I changed `alpha` to 15. \n",
    "- Hybrid PCA: first apply full PCA to extract `n_components`. Then calculate: \n",
    "```\n",
    "reconstructed_data = self.pca.inverse_transform(pca_transformed)\n",
    "model_residuals = original_data - reconstructed_data\n",
    "``` \n",
    "Then apply sparse PCA as above to the matrix `model_residuals`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
